{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 200743,
          "sourceType": "datasetVersion",
          "datasetId": 87153
        }
      ],
      "dockerImageVersionId": 30528,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Malaria Detection |efficientnetB3|Acc:97% - Curent",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesis17/Afaan-Oromoo-chatGPT/blob/main/Malaria_Detection_%7CefficientnetB3%7CAcc_97_Curent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "iarunava_cell_images_for_detecting_malaria_path = kagglehub.dataset_download('iarunava/cell-images-for-detecting-malaria')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "oT24wfUMmITt"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "mQY8v4FAmITu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy Images"
      ],
      "metadata": {
        "id": "WcZlkzeXmITu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print Number of Images per class\n",
        "import os\n",
        "\n",
        "# Path to your dataset folder\n",
        "base_dir = \"/kaggle/input/cell-images-for-detecting-malaria/cell_images\"\n",
        "\n",
        "# Loop through each subfolder\n",
        "for folder in [\"Parasitized\", \"Uninfected\"]:\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        count = len([\n",
        "            f for f in os.listdir(folder_path)\n",
        "            if os.path.isfile(os.path.join(folder_path, f))\n",
        "        ])\n",
        "        print(f\"{folder}: {count} images\")\n",
        "    else:\n",
        "        print(f\"{folder}: Directory not found\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T22:39:02.686232Z",
          "iopub.execute_input": "2025-08-08T22:39:02.68658Z",
          "iopub.status.idle": "2025-08-08T22:40:04.428203Z",
          "shell.execute_reply.started": "2025-08-08T22:39:02.686553Z",
          "shell.execute_reply": "2025-08-08T22:40:04.427169Z"
        },
        "id": "ta1IPch_mITv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the source directory (where 'cell_images' is located)\n",
        "# Assuming 'cell_images' is in the current directory where you run the script\n",
        "source_root_dir = '.'\n",
        "\n",
        "# Define the main destination directory\n",
        "destination_root_dir = '/kaggle/working/cell-images'\n",
        "\n",
        "# Define the paths to the original Paracitized and Uninfected folders\n",
        "paracitized_source_folder = os.path.join(source_root_dir, '/kaggle/input/cell-images-for-detecting-malaria/cell_images', 'Parasitized')\n",
        "uninfected_source_folder = os.path.join(source_root_dir, '/kaggle/input/cell-images-for-detecting-malaria/cell_images', 'Uninfected')\n",
        "\n",
        "# Define the target subdirectories within 'rumman'\n",
        "paracitized_dest_folder = os.path.join(destination_root_dir, 'Parasitized') # Corrected spelling to match user request\n",
        "uninfected_dest_folder = os.path.join(destination_root_dir, 'Uninfected')\n",
        "\n",
        "# Create the main destination directory and its subdirectories if they don't exist\n",
        "os.makedirs(paracitized_dest_folder, exist_ok=True)\n",
        "os.makedirs(uninfected_dest_folder, exist_ok=True)\n",
        "\n",
        "def copy_random_images(source_folder, dest_folder, num_to_copy):\n",
        "    \"\"\"\n",
        "    Copies a specified number of random images from source_folder to dest_folder.\n",
        "    \"\"\"\n",
        "    # Get all image files (assuming files without extensions are not images or we filter them later)\n",
        "    all_images = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "\n",
        "    # Shuffle the list of images to get a random selection\n",
        "    random.shuffle(all_images)\n",
        "\n",
        "    # Select up to num_to_copy images. If source has fewer, it copies all available.\n",
        "    images_to_copy = all_images[:num_to_copy]\n",
        "\n",
        "    print(f\"Copying {len(images_to_copy)} images from '{source_folder}' to '{dest_folder}'...\")\n",
        "    for image_name in images_to_copy:\n",
        "        src_path = os.path.join(source_folder, image_name)\n",
        "        dest_path = os.path.join(dest_folder, image_name)\n",
        "        try:\n",
        "            shutil.copy(src_path, dest_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying '{image_name}': {e}\")\n",
        "    print(f\"Finished copying from '{source_folder}'.\\n\")\n",
        "\n",
        "# Number of images to copy from each category\n",
        "num_images_per_category = 1000\n",
        "\n",
        "# Copy images to the 'paratized' folder inside 'rumman'\n",
        "copy_random_images(paracitized_source_folder, paracitized_dest_folder, num_images_per_category)\n",
        "\n",
        "# Copy images to the 'uninfected' folder inside 'rumman'\n",
        "copy_random_images(uninfected_source_folder, uninfected_dest_folder, num_images_per_category)\n",
        "\n",
        "print(f\"Image copying complete. Check the '{destination_root_dir}' folder.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T01:07:53.738443Z",
          "iopub.execute_input": "2025-08-09T01:07:53.739154Z",
          "iopub.status.idle": "2025-08-09T01:09:22.396431Z",
          "shell.execute_reply.started": "2025-08-09T01:07:53.739115Z",
          "shell.execute_reply": "2025-08-09T01:09:22.395589Z"
        },
        "id": "BhiC9W1rmITv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0JcstioVmITw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model-1 (BBHE + EffecientNetB3 + MPA + SVM)"
      ],
      "metadata": {
        "id": "ucPJokvdmITw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "def apply_bbhe(image):\n",
        "    \"\"\"\n",
        "    Applies Brightness Preserving Bi-Histogram Equalization (BBHE) to an image.\n",
        "    This implementation is a simplified representation. A full BBHE implementation\n",
        "    is more complex, often involving median calculations for sub-histograms.\n",
        "    For practical purposes, a combination of CLAHE or standard HE might be used\n",
        "    if a precise BBHE library isn't readily available for direct application.\n",
        "    Here, we'll demonstrate a basic adaptive equalization that can achieve\n",
        "    similar goals of contrast enhancement.\n",
        "    \"\"\"\n",
        "    # Convert image to LAB color space\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "    # Split the LAB image into L, A, and B channels\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # Apply CLAHE to the L-channel (luminosity)\n",
        "    # This is a common and effective adaptive histogram equalization technique\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge the enhanced L-channel with the original A and B channels\n",
        "    merged_lab = cv2.merge([cl, a, b])\n",
        "\n",
        "    # Convert back to BGR color space\n",
        "    enhanced_image = cv2.cvtColor(merged_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    return enhanced_image\n",
        "\n",
        "def preprocess_dataset_with_bbhe(input_base_dir, output_base_dir):\n",
        "    \"\"\"\n",
        "    Processes all images in the input directory, applies BBHE, and saves them\n",
        "    to the output directory maintaining the original folder structure.\n",
        "    \"\"\"\n",
        "    for class_name in ['Parasitized', 'Uninfected']:\n",
        "        input_class_dir = os.path.join(input_base_dir, class_name)\n",
        "        output_class_dir = os.path.join(output_base_dir, class_name)\n",
        "\n",
        "        os.makedirs(output_class_dir, exist_ok=True)\n",
        "\n",
        "        image_files = [f for f in os.listdir(input_class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        print(f\"Processing {class_name} images...\")\n",
        "        for img_name in tqdm(image_files):\n",
        "            img_path = os.path.join(input_class_dir, img_name)\n",
        "            output_path = os.path.join(output_class_dir, img_name)\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            enhanced_img = apply_bbhe(img)\n",
        "            cv2.imwrite(output_path, enhanced_img)\n",
        "    print(\"Data Preprocessing complete. Enhanced images saved.\")\n",
        "\n",
        "# --- How to use ---\n",
        "# Assuming 'cell_images' is in your current working directory\n",
        "original_dataset_path = '/kaggle/working/cell-images'\n",
        "enhanced_dataset_path = '/kaggle/working/cell_images_enhanced_bbhe'\n",
        "preprocess_dataset_with_bbhe(original_dataset_path, enhanced_dataset_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:08:58.000264Z",
          "iopub.execute_input": "2025-08-18T04:08:58.000963Z",
          "iopub.status.idle": "2025-08-18T04:08:58.602986Z",
          "shell.execute_reply.started": "2025-08-18T04:08:58.000935Z",
          "shell.execute_reply": "2025-08-18T04:08:58.601802Z"
        },
        "id": "KksPJ8oMmITw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(dataset_path, model):\n",
        "    \"\"\"\n",
        "    Extracts features from images in a given dataset path using the provided model.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for class_name in ['Parasitized', 'Uninfected']:\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        print(f\"Extracting features from {class_name} in {dataset_path}...\")\n",
        "        for img_name in tqdm(image_files):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "            # Load and preprocess image for EfficientNet\n",
        "            img = load_img(img_path, target_size=(224, 224)) # EfficientNetB0 input size\n",
        "            img_array = img_to_array(img)\n",
        "            img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
        "            img_array = preprocess_input(img_array) # EfficientNet specific preprocessing\n",
        "\n",
        "            # Get features\n",
        "            feature_vector = model.predict(img_array, verbose=0).flatten()\n",
        "            features.append(feature_vector)\n",
        "            labels.append(0 if class_name == 'Uninfected' else 1) # 0 for Uninfected, 1 for Parasitized\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# --- How to use ---\n",
        "# Load pre-trained EfficientNetB0 model without the top classification layer\n",
        "# This creates a feature extractor\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False, pooling='avg') # 'avg' for global average pooling\n",
        "\n",
        "original_features, original_labels = extract_features('/kaggle/working/cell-images', base_model)\n",
        "enhanced_features, enhanced_labels = extract_features('/kaggle/working/cell_images_enhanced_bbhe', base_model)\n",
        "\n",
        "print(f\"Original features shape: {original_features.shape}\")\n",
        "print(f\"Enhanced features shape: {enhanced_features.shape}\")\n",
        "\n",
        "# Save features to disk for later use\n",
        "# np.save('original_features.npy', original_features)\n",
        "# np.save('original_labels.npy', original_labels)\n",
        "# np.save('enhanced_features.npy', enhanced_features)\n",
        "# np.save('enhanced_labels.npy', enhanced_labels)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CXvQTTScmITx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fuse_features(features1, features2):\n",
        "    \"\"\"\n",
        "    Concatenates two feature sets. Assumes features are aligned (from the same images).\n",
        "    \"\"\"\n",
        "    # Ensure they have the same number of samples\n",
        "    if features1.shape[0] != features2.shape[0]:\n",
        "        raise ValueError(\"Feature sets must have the same number of samples for concatenation.\")\n",
        "\n",
        "    fused_features = np.concatenate((features1, features2), axis=1)\n",
        "    return fused_features\n",
        "\n",
        "# --- How to use ---\n",
        "fused_features = fuse_features(original_features, enhanced_features)\n",
        "print(f\"Fused features shape: {fused_features.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dbpu6GD0mITx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.array_equal(original_labels, enhanced_labels)  # Should return True"
      ],
      "metadata": {
        "trusted": true,
        "id": "fPEjAxAwmITx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def objective_function(feature_mask, X_data, y_data):\n",
        "    binary_mask = np.array(feature_mask) > 0.5\n",
        "\n",
        "    if np.sum(binary_mask) == 0:\n",
        "        return 0.0  # Avoid empty feature subset\n",
        "\n",
        "    selected_X = X_data[:, binary_mask]\n",
        "    scaler = StandardScaler()\n",
        "    selected_X_scaled = scaler.fit_transform(selected_X)\n",
        "\n",
        "    model = SVC(kernel='rbf', random_state=42)\n",
        "    scores = cross_val_score(model, selected_X_scaled, y_data, cv=5, scoring='accuracy')\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n",
        "def simple_mpa(X, y, n_agents=10, n_iter=20):\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    agents = np.random.rand(n_agents, n_features)\n",
        "    fitness = np.array([objective_function(agent, X, y) for agent in agents])\n",
        "\n",
        "    best_idx = np.argmax(fitness)\n",
        "    best_agent = agents[best_idx].copy()\n",
        "    best_score = fitness[best_idx]\n",
        "\n",
        "    for iteration in range(n_iter):\n",
        "        for i in range(n_agents):\n",
        "            r1, r2 = np.random.rand(n_features), np.random.rand(n_features)\n",
        "            step = r1 * (best_agent - agents[i]) + r2 * (agents[i] - best_agent)\n",
        "            agents[i] += step * np.random.normal(0, 0.1, n_features)\n",
        "            agents[i] = np.clip(agents[i], 0, 1)\n",
        "\n",
        "        fitness = np.array([objective_function(agent, X, y) for agent in agents])\n",
        "        best_idx = np.argmax(fitness)\n",
        "        if fitness[best_idx] > best_score:\n",
        "            best_agent = agents[best_idx].copy()\n",
        "            best_score = fitness[best_idx]\n",
        "\n",
        "        print(f\"Iteration {iteration+1}/{n_iter} - Best Accuracy: {best_score:.4f}\")\n",
        "\n",
        "    return best_agent > 0.5\n",
        "\n",
        "# Make sure you already have:\n",
        "labels = original_labels\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print('start')\n",
        "selected_mask = simple_mpa(fused_features, labels, n_agents=15, n_iter=5)\n",
        "selected_features = fused_features[:, selected_mask]\n",
        "\n",
        "print('End')\n",
        "end = time.time()\n",
        "print(end-start)\n",
        "\n",
        "print(f\"Selected {np.sum(selected_mask)} features out of {fused_features.shape[1]}\")\n",
        "print(f\"Selected feature shape: {selected_features.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "N8eyp_63mITy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, roc_auc_score, ConfusionMatrixDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Assuming selected_features and original_labels are available\n",
        "X = selected_features\n",
        "y = original_labels\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "performance_metrics = {}\n",
        "\n",
        "print(\"\\n--- Training and Evaluating SVM Classifiers ---\")\n",
        "for kernel in kernels:\n",
        "    print(f\"Training SVM with {kernel} kernel...\")\n",
        "    svm = SVC(kernel=kernel, probability=True, random_state=42)\n",
        "\n",
        "    # Training\n",
        "    start_time = time.time()\n",
        "    svm.fit(X_train_scaled, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Prediction\n",
        "    start_time = time.time()\n",
        "    y_pred = svm.predict(X_test_scaled)\n",
        "    prediction_speed = (time.time() - start_time) / len(y_test)\n",
        "\n",
        "    # Probabilities for ROC–AUC\n",
        "    y_proba = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    performance_metrics[kernel] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': auc,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'training_time': training_time,\n",
        "        'prediction_speed_per_sample': prediction_speed,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1 Score: {f1:.4f}\")\n",
        "    print(f\"  ROC–AUC: {auc:.4f}\")\n",
        "    print(f\"  Training Time: {training_time:.2f} sec\")\n",
        "    print(f\"  Prediction Speed: {prediction_speed:.6f} sec/sample\")\n",
        "    print(f\"  Confusion Matrix:\\n{cm}\\n\")\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix ({kernel} kernel)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.plot(fpr, tpr, label=f\"{kernel} (AUC={auc:.4f})\")\n",
        "\n",
        "# Final ROC Curve graph for all kernels\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve - SVM Kernels\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lz4sB4F-mITy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "4XyHDDRhmITy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BBHE + EffeceientNetB3 + SVM ( Without MPA)"
      ],
      "metadata": {
        "id": "gtVGHSs3mITy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# BBHE Function\n",
        "# ---------------------------\n",
        "def apply_bbhe(image):\n",
        "    # Convert image to LAB color space\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "    # Split channels\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # CLAHE on L-channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge and convert back to BGR\n",
        "    merged_lab = cv2.merge([cl, a, b])\n",
        "    enhanced_image = cv2.cvtColor(merged_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    return enhanced_image\n",
        "\n",
        "def preprocess_dataset_with_bbhe(input_base_dir, output_base_dir):\n",
        "    for class_name in ['Parasitized', 'Uninfected']:\n",
        "        input_class_dir = os.path.join(input_base_dir, class_name)\n",
        "        output_class_dir = os.path.join(output_base_dir, class_name)\n",
        "\n",
        "        os.makedirs(output_class_dir, exist_ok=True)\n",
        "\n",
        "        image_files = [f for f in os.listdir(input_class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        print(f\"Processing {class_name} images...\")\n",
        "        for img_name in tqdm(image_files):\n",
        "            img_path = os.path.join(input_class_dir, img_name)\n",
        "            output_path = os.path.join(output_class_dir, img_name)\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not read image {img_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            enhanced_img = apply_bbhe(img)\n",
        "            cv2.imwrite(output_path, enhanced_img)\n",
        "    print(\"Data Preprocessing complete. Enhanced images saved.\")\n",
        "\n",
        "# Example usage\n",
        "original_dataset_path = '/kaggle/input/cell-images-for-detecting-malaria/cell_images'\n",
        "enhanced_dataset_path = '/kaggle/working/cell_images_enhanced_bbhe'\n",
        "preprocess_dataset_with_bbhe(original_dataset_path, enhanced_dataset_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T20:37:46.159425Z",
          "iopub.execute_input": "2025-08-18T20:37:46.159661Z",
          "iopub.status.idle": "2025-08-18T20:43:49.826846Z",
          "shell.execute_reply.started": "2025-08-18T20:37:46.159639Z",
          "shell.execute_reply": "2025-08-18T20:43:49.825995Z"
        },
        "id": "8cmTAAy7mITy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Feature Extraction\n",
        "# ---------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "def extract_features(dataset_path, model):\n",
        "    features, labels = [], []\n",
        "\n",
        "    for class_name in ['Parasitized', 'Uninfected']:\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        print(f\"Extracting features from {class_name} in {dataset_path}...\")\n",
        "        for img_name in tqdm(image_files):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "            img = load_img(img_path, target_size=(224, 224))\n",
        "            img_array = img_to_array(img)\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "            img_array = preprocess_input(img_array)\n",
        "\n",
        "            feature_vector = model.predict(img_array, verbose=0).flatten()\n",
        "            features.append(feature_vector)\n",
        "            labels.append(0 if class_name == 'Uninfected' else 1)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False, pooling='avg')\n",
        "# base_model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T20:43:49.828541Z",
          "iopub.execute_input": "2025-08-18T20:43:49.828791Z",
          "iopub.status.idle": "2025-08-18T20:44:03.39859Z",
          "shell.execute_reply.started": "2025-08-18T20:43:49.828769Z",
          "shell.execute_reply": "2025-08-18T20:44:03.39789Z"
        },
        "id": "bjOw_zlAmITy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "original_features, original_labels = extract_features(original_dataset_path, base_model)\n",
        "enhanced_features, enhanced_labels = extract_features(enhanced_dataset_path, base_model)\n",
        "\n",
        "print(f\"Original features shape: {original_features.shape}\")\n",
        "print(f\"Enhanced features shape: {enhanced_features.shape}\")\n",
        "\n",
        "# Fuse both sets of features (optional)\n",
        "fused_features = np.concatenate((original_features, enhanced_features), axis=1)\n",
        "labels = original_labels  # Assuming same order of images"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T20:44:03.399558Z",
          "iopub.execute_input": "2025-08-18T20:44:03.399782Z"
        },
        "id": "AuiE5zzCmITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Train and Evaluate SVM\n",
        "# ---------------------------\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, roc_auc_score, ConfusionMatrixDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "X = fused_features\n",
        "y = labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "kernels = ['poly']\n",
        "performance_metrics = {}\n",
        "\n",
        "print(\"\\n--- Training and Evaluating SVM Classifiers ---\")\n",
        "for kernel in kernels:\n",
        "    print(f\"Training SVM with {kernel} kernel...\")\n",
        "    svm = SVC(kernel=kernel, probability=True, random_state=42)\n",
        "\n",
        "    start_time = time.time()\n",
        "    svm.fit(X_train_scaled, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred = svm.predict(X_test_scaled)\n",
        "    prediction_speed = (time.time() - start_time) / len(y_test)\n",
        "\n",
        "    y_proba = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    performance_metrics[kernel] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': auc,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'training_time': training_time,\n",
        "        'prediction_speed_per_sample': prediction_speed,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1 Score: {f1:.4f}\")\n",
        "    print(f\"  ROC–AUC: {auc:.4f}\")\n",
        "    print(f\"  Training Time: {training_time:.2f} sec\")\n",
        "    print(f\"  Prediction Speed: {prediction_speed:.6f} sec/sample\")\n",
        "    print(f\"  Confusion Matrix:\\n{cm}\\n\")\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix ({kernel} kernel)\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f\"{kernel} (AUC={auc:.4f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"ROC Curve - SVM Kernels\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-08-14T00:20:47.226Z"
        },
        "id": "69qsF2RWmITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "3ZdY1xu3mITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet121"
      ],
      "metadata": {
        "id": "0mWQmqymmITz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import system libs\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "import itertools\n",
        "from PIL import Image\n",
        "# import data handling tools\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:12:26.852674Z",
          "iopub.execute_input": "2025-08-18T04:12:26.853339Z",
          "iopub.status.idle": "2025-08-18T04:12:27.423548Z",
          "shell.execute_reply.started": "2025-08-18T04:12:26.853309Z",
          "shell.execute_reply": "2025-08-18T04:12:27.422581Z"
        },
        "id": "R1plm3RFmITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')\n",
        "\n",
        "# Generate data paths with labels\n",
        "data_dir = '/kaggle/input/cell-images-for-detecting-malaria/cell_images'\n",
        "filepaths = []\n",
        "labels = []\n",
        "\n",
        "folds = os.listdir(data_dir)\n",
        "for fold in folds:\n",
        "    foldpath = os.path.join(data_dir, fold)\n",
        "    filelist = os.listdir(foldpath)\n",
        "    for file in filelist:\n",
        "        fpath = os.path.join(foldpath, file)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(fold)\n",
        "\n",
        "# Concatenate data paths with labels into one dataframe\n",
        "Fseries = pd.Series(filepaths, name= 'filepaths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "df = pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "\n",
        "# MODIFIED: Split data into train and test sets (80/20 split)\n",
        "train_df, test_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123)\n",
        "\n",
        "\n",
        "# crobed image size\n",
        "batch_size = 64\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "# Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
        "ts_length = len(test_df)\n",
        "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "\n",
        "# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
        "def scalar(img):\n",
        "    return img\n",
        "\n",
        "tr_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "\n",
        "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                     color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "\n",
        "# REMOVED: valid_gen is no longer needed\n",
        "\n",
        "# Note: we will use custom test_batch_size, and make shuffle= false\n",
        "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                     color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n",
        "\n",
        "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
        "classes = list(g_dict.keys())         # defines list of dictionary's kays (classes), classes names : string\n",
        "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
        "\n",
        "# calculate number of displayed samples\n",
        "length = len(labels)      # length of batch size\n",
        "sample = min(length, 25)  # check if sample less than 25 images"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:12:30.093438Z",
          "iopub.execute_input": "2025-08-18T04:12:30.093776Z",
          "iopub.status.idle": "2025-08-18T04:13:11.030968Z",
          "shell.execute_reply.started": "2025-08-18T04:12:30.093746Z",
          "shell.execute_reply": "2025-08-18T04:13:11.029996Z"
        },
        "id": "WLIpJoHWmITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (20, 20))\n",
        "for i in range(sample):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    image = images[i] / 255       # scales data to range (0 - 1) for plotting\n",
        "    plt.imshow(image)\n",
        "    index = np.argmax(labels[i])  # get image index\n",
        "    class_name = classes[index]   # get class of image\n",
        "    plt.title(class_name, color= 'blue', fontsize= 12)\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:13:11.032457Z",
          "iopub.execute_input": "2025-08-18T04:13:11.03279Z",
          "iopub.status.idle": "2025-08-18T04:13:14.774211Z",
          "shell.execute_reply.started": "2025-08-18T04:13:11.032765Z",
          "shell.execute_reply": "2025-08-18T04:13:14.773117Z"
        },
        "id": "lFRly0tzmITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
        "# we will use efficientnetb3 from EfficientNet family.\n",
        "# base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "# Base model: DenseNet121\n",
        "base_model = tf.keras.applications.DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=img_shape,\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "           bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:26:27.94722Z",
          "iopub.execute_input": "2025-08-09T23:26:27.947508Z",
          "iopub.status.idle": "2025-08-09T23:26:36.50351Z",
          "shell.execute_reply.started": "2025-08-09T23:26:27.947484Z",
          "shell.execute_reply": "2025-08-09T23:26:36.502518Z"
        },
        "id": "OcFap697mITz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Save and plot the model structure\n",
        "plot_model(\n",
        "    model,\n",
        "    to_file='DenseNet121.png',\n",
        "    show_shapes=True,       # Show tensor shapes\n",
        "    show_layer_names=True,  # Show layer names\n",
        "    expand_nested=False,     # Expand nested models (like DenseNet)\n",
        "    dpi=96\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:26:36.5048Z",
          "iopub.execute_input": "2025-08-09T23:26:36.505147Z",
          "iopub.status.idle": "2025-08-09T23:26:36.750053Z",
          "shell.execute_reply.started": "2025-08-09T23:26:36.505114Z",
          "shell.execute_reply": "2025-08-09T23:26:36.749133Z"
        },
        "id": "re-ThGmzmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 1. Create a checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='densenet.h5',       # Save model as densenet.h5\n",
        "    monitor='val_accuracy',       # Monitor validation accuracy\n",
        "    save_best_only=True,          # Save only when best val_accuracy\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "epochs = 10  # number of all epochs in training\n",
        "\n",
        "# 2. Train and save best model\n",
        "history = model.fit(\n",
        "    x=train_gen,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=test_gen,\n",
        "    validation_steps=None,\n",
        "    shuffle=False,\n",
        "    # callbacks=[checkpoint]  # <-- Saving best model\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:26:36.751447Z",
          "iopub.execute_input": "2025-08-09T23:26:36.751767Z",
          "iopub.status.idle": "2025-08-09T23:51:24.881333Z",
          "shell.execute_reply.started": "2025-08-09T23:26:36.751738Z",
          "shell.execute_reply": "2025-08-09T23:51:24.880568Z"
        },
        "id": "1fOFqMC9mIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3. Load the best saved model\n",
        "# best_model = load_model('densenet.h5')\n",
        "\n",
        "# # 4. Evaluate on test data\n",
        "# loss, acc = best_model.evaluate(test_gen, verbose=1)\n",
        "# print(f\"Best model accuracy: {acc*100:.2f}%\")\n",
        "# print(f\"Best model loss: {loss:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bHxXhm_5mIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1️⃣ Predict on the test set\n",
        "y_pred_proba = model.predict(test_gen)  # Probabilities\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)  # Predicted class indices\n",
        "y_true = test_gen.classes  # True labels\n",
        "\n",
        "# If binary classification, convert probabilities for ROC–AUC\n",
        "if len(test_gen.class_indices) == 2:\n",
        "    y_pred_binary = y_pred_proba[:, 1]\n",
        "else:\n",
        "    y_pred_binary = None  # ROC–AUC for multi-class needs special handling"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T00:17:55.373181Z",
          "iopub.execute_input": "2025-08-10T00:17:55.373525Z",
          "iopub.status.idle": "2025-08-10T00:18:12.003142Z",
          "shell.execute_reply.started": "2025-08-10T00:17:55.37348Z",
          "shell.execute_reply": "2025-08-10T00:18:12.002405Z"
        },
        "id": "XA3Mda72mIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "if y_pred_binary is not None:\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_binary)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_binary)\n",
        "    print(f\"ROC–AUC:   {roc_auc:.4f}\")\n",
        "else:\n",
        "    print(\"ROC–AUC and FPR not computed for multi-class directly.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T00:20:35.774804Z",
          "iopub.execute_input": "2025-08-10T00:20:35.775515Z",
          "iopub.status.idle": "2025-08-10T00:20:35.808557Z",
          "shell.execute_reply.started": "2025-08-10T00:20:35.775485Z",
          "shell.execute_reply": "2025-08-10T00:20:35.807726Z"
        },
        "id": "ib9ChGxnmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "AX8BdjlCmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QdoyKvuVmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "O1ikTWBUmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc = 0.9890"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T00:24:18.140565Z",
          "iopub.execute_input": "2025-08-10T00:24:18.141315Z",
          "iopub.status.idle": "2025-08-10T00:24:18.145015Z",
          "shell.execute_reply.started": "2025-08-10T00:24:18.141287Z",
          "shell.execute_reply": "2025-08-10T00:24:18.144074Z"
        },
        "id": "7sHT4EDpmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Pc9jSEvFmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CDg2auiMmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tr_acc)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:58:23.011975Z",
          "iopub.execute_input": "2025-08-09T23:58:23.012565Z",
          "iopub.status.idle": "2025-08-09T23:58:23.016959Z",
          "shell.execute_reply.started": "2025-08-09T23:58:23.012531Z",
          "shell.execute_reply": "2025-08-09T23:58:23.016053Z"
        },
        "id": "FPg6b2WgmIT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_acc)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:58:36.099419Z",
          "iopub.execute_input": "2025-08-09T23:58:36.100055Z",
          "iopub.status.idle": "2025-08-09T23:58:36.104397Z",
          "shell.execute_reply.started": "2025-08-09T23:58:36.100024Z",
          "shell.execute_reply": "2025-08-09T23:58:36.103448Z"
        },
        "id": "8bwrpPk-mIT1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data from the extracted values\n",
        "training_accuracy = [0.954865038394928, 0.9676570892333984, 0.9715127944946289, 0.9735540747642517, 0.9764572381973267, 0.9776366353034973, 0.9800862073898315, 0.9832615256309509, 0.9856203198432922, 0.9875255227088928]\n",
        "validation_accuracy = [0.969345211982727, 0.9682568311691284, 0.9668057560920715, 0.9711591005325317, 0.9695265889167786, 0.9736985564231873, 0.973517119884491, 0.9657173752784729, 0.9697079658508301, 0.9689823985099792]\n",
        "\n",
        "epochs = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "# Plot with larger figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, training_accuracy, 'r-', linewidth=2, label='Training Accuracy')\n",
        "plt.plot(epochs, validation_accuracy, 'g-', linewidth=2, label='Validation Accuracy')\n",
        "\n",
        "# Highlight best epoch\n",
        "best_epoch = validation_accuracy.index(max(validation_accuracy)) + 1\n",
        "plt.scatter(best_epoch, max(validation_accuracy), color='blue', s=100, label=f'best epoch= {best_epoch}')\n",
        "\n",
        "# Labels and title\n",
        "plt.title('Training and Validation Accuracy', fontsize=16)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:58:43.35911Z",
          "iopub.execute_input": "2025-08-09T23:58:43.35984Z",
          "iopub.status.idle": "2025-08-09T23:58:43.712752Z",
          "shell.execute_reply.started": "2025-08-09T23:58:43.359808Z",
          "shell.execute_reply": "2025-08-09T23:58:43.71176Z"
        },
        "id": "FhJQVvUMmIT1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now write about Drouptout layer without any plagrism and AI detection and it should look like more human written"
      ],
      "metadata": {
        "trusted": true,
        "id": "SDFsHmyZmIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "vqlD3pH0mIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EffecientNetB3"
      ],
      "metadata": {
        "id": "7ZccFFtvmIT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import system libs\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "import itertools\n",
        "from PIL import Image\n",
        "# import data handling tools\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:13:14.775641Z",
          "iopub.execute_input": "2025-08-18T04:13:14.775905Z",
          "iopub.status.idle": "2025-08-18T04:13:14.781824Z",
          "shell.execute_reply.started": "2025-08-18T04:13:14.775882Z",
          "shell.execute_reply": "2025-08-18T04:13:14.781049Z"
        },
        "id": "Tcm-9kRImIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')\n",
        "\n",
        "# Generate data paths with labels\n",
        "data_dir = '/kaggle/input/cell-images-for-detecting-malaria/cell_images'\n",
        "filepaths = []\n",
        "labels = []\n",
        "\n",
        "folds = os.listdir(data_dir)\n",
        "for fold in folds:\n",
        "    foldpath = os.path.join(data_dir, fold)\n",
        "    filelist = os.listdir(foldpath)\n",
        "    for file in filelist:\n",
        "        fpath = os.path.join(foldpath, file)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(fold)\n",
        "\n",
        "# Concatenate data paths with labels into one dataframe\n",
        "Fseries = pd.Series(filepaths, name= 'filepaths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "df = pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "\n",
        "# MODIFIED: Split data into train and test sets (80/20 split)\n",
        "train_df, test_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123)\n",
        "\n",
        "\n",
        "# crobed image size\n",
        "batch_size = 64\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "# Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
        "ts_length = len(test_df)\n",
        "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "\n",
        "# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
        "def scalar(img):\n",
        "    return img\n",
        "\n",
        "tr_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "\n",
        "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                     color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "\n",
        "# REMOVED: valid_gen is no longer needed\n",
        "\n",
        "# Note: we will use custom test_batch_size, and make shuffle= false\n",
        "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                     color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n",
        "\n",
        "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
        "classes = list(g_dict.keys())         # defines list of dictionary's kays (classes), classes names : string\n",
        "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
        "\n",
        "# calculate number of displayed samples\n",
        "length = len(labels)      # length of batch size\n",
        "sample = min(length, 25)  # check if sample less than 25 images"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:13:15.914683Z",
          "iopub.execute_input": "2025-08-18T04:13:15.915077Z",
          "iopub.status.idle": "2025-08-18T04:13:29.477077Z",
          "shell.execute_reply.started": "2025-08-18T04:13:15.915042Z",
          "shell.execute_reply": "2025-08-18T04:13:29.476394Z"
        },
        "id": "hT4OJbY0mIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Count for each class in train set\n",
        "train_counts = train_df['labels'].value_counts()\n",
        "\n",
        "# Count for each class in test set\n",
        "test_counts = test_df['labels'].value_counts()\n",
        "\n",
        "# Fill the table\n",
        "for cls in classes:\n",
        "    train_count = train_counts.get(cls, 0)\n",
        "    test_count = test_counts.get(cls, 0)\n",
        "    total_count = train_count + test_count\n",
        "    print(f\"{cls} | Train: {train_count} | Test: {test_count} | Total: {total_count}\")\n",
        "\n",
        "# Overall counts\n",
        "overall_train = len(train_df)\n",
        "overall_test = len(test_df)\n",
        "overall_total = overall_train + overall_test\n",
        "print(f\"Overall | Train: {overall_train} | Test: {overall_test} | Total: {overall_total}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:13:29.478653Z",
          "iopub.execute_input": "2025-08-18T04:13:29.478994Z",
          "iopub.status.idle": "2025-08-18T04:13:29.491879Z",
          "shell.execute_reply.started": "2025-08-18T04:13:29.478962Z",
          "shell.execute_reply": "2025-08-18T04:13:29.491138Z"
        },
        "id": "bSIBJ9gpmIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cy1IWXzVmIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
        "# we will use efficientnetb3 from EfficientNet family.\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "           bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T23:46:31.060557Z",
          "iopub.execute_input": "2025-08-08T23:46:31.061329Z",
          "iopub.status.idle": "2025-08-08T23:46:39.079339Z",
          "shell.execute_reply.started": "2025-08-08T23:46:31.061297Z",
          "shell.execute_reply": "2025-08-08T23:46:39.078418Z"
        },
        "id": "LP-htq_QmIT4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Save and plot the model structure\n",
        "plot_model(\n",
        "    model,\n",
        "    to_file='EffecientNetB3.png',\n",
        "    show_shapes=True,       # Show tensor shapes\n",
        "    show_layer_names=True,  # Show layer names\n",
        "    expand_nested=False,     # Expand nested models (like DenseNet)\n",
        "    dpi=96\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T23:46:39.080883Z",
          "iopub.execute_input": "2025-08-08T23:46:39.081221Z",
          "iopub.status.idle": "2025-08-08T23:46:39.16157Z",
          "shell.execute_reply.started": "2025-08-08T23:46:39.081193Z",
          "shell.execute_reply": "2025-08-08T23:46:39.16055Z"
        },
        "id": "1Z9tsQCEmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# # 1. Create a checkpoint to save the best model\n",
        "# checkpoint = ModelCheckpoint(\n",
        "#     filepath='effecientNet.h5',       # Save model as densenet.h5\n",
        "#     monitor='val_accuracy',       # Monitor validation accuracy\n",
        "#     save_best_only=True,          # Save only when best val_accuracy\n",
        "#     mode='max',\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "epochs = 10  # number of all epochs in training\n",
        "\n",
        "# 2. Train and save best model\n",
        "history = model.fit(\n",
        "    x=train_gen,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data=test_gen,\n",
        "    validation_steps=None,\n",
        "    shuffle=False,\n",
        "    # callbacks=[checkpoint]  # <-- Saving best model\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T18:24:04.20819Z",
          "iopub.execute_input": "2025-08-08T18:24:04.208451Z",
          "iopub.status.idle": "2025-08-08T19:09:38.355732Z",
          "shell.execute_reply.started": "2025-08-08T18:24:04.208427Z",
          "shell.execute_reply": "2025-08-08T19:09:38.354737Z"
        },
        "id": "nxOFoqGJmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load the best saved model\n",
        "best_model = load_model('effecientNet.h5')\n",
        "\n",
        "# 4. Evaluate on test data\n",
        "loss, acc = best_model.evaluate(test_gen, verbose=1)\n",
        "print(f\"Best model accuracy: {acc*100:.2f}%\")\n",
        "print(f\"Best model loss: {loss:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4JyN9bNRmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
        ")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1️⃣ Predict on the test set\n",
        "y_pred_proba = model.predict(test_gen)  # Probabilities\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)  # Predicted class indices\n",
        "y_true = test_gen.classes  # True labels\n",
        "\n",
        "# If binary classification, convert probabilities for ROC–AUC\n",
        "if len(test_gen.class_indices) == 2:\n",
        "    y_pred_binary = y_pred_proba[:, 1]\n",
        "else:\n",
        "    y_pred_binary = None  # ROC–AUC for multi-class needs special handling"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T19:09:38.357799Z",
          "iopub.execute_input": "2025-08-08T19:09:38.358537Z",
          "iopub.status.idle": "2025-08-08T19:09:52.688324Z",
          "shell.execute_reply.started": "2025-08-08T19:09:38.358496Z",
          "shell.execute_reply": "2025-08-08T19:09:52.687546Z"
        },
        "id": "vvtP67tXmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TfluVvo7mIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2️⃣ Metrics\n",
        "# accuracy = accuracy_score(y_true, y_pred)\n",
        "# precision = precision_score(y_true, y_pred, average='weighted')\n",
        "# recall = recall_score(y_true, y_pred, average='weighted')\n",
        "# f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "# print(f\"Precision: {precision:.4f}\")\n",
        "# print(f\"Recall:    {recall:.4f}\")\n",
        "# print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "# if y_pred_binary is not None:\n",
        "#     roc_auc = roc_auc_score(y_true, y_pred_binary)\n",
        "#     fpr, tpr, thresholds = roc_curve(y_true, y_pred_binary)\n",
        "#     print(f\"ROC–AUC:   {roc_auc:.4f}\")\n",
        "# else:\n",
        "#     print(\"ROC–AUC and FPR not computed for multi-class directly.\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Confusion matrix values\n",
        "cm = np.array([[2655, 69],\n",
        "               [50, 2739]])\n",
        "\n",
        "# Extract values\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Create true labels and predicted labels\n",
        "y_true = [0]* (tn + fp) + [1]* (fn + tp)\n",
        "y_pred = [0]* tn + [1]* fp + [0]* fn + [1]* tp\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:10:58.662211Z",
          "iopub.execute_input": "2025-08-09T23:10:58.662863Z",
          "iopub.status.idle": "2025-08-09T23:10:58.895024Z",
          "shell.execute_reply.started": "2025-08-09T23:10:58.662828Z",
          "shell.execute_reply": "2025-08-09T23:10:58.894006Z"
        },
        "id": "61o5-u3PmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 3️⃣ Confusion Matrix\n",
        "# cm = confusion_matrix(y_true, y_pred)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_gen.class_indices.keys())\n",
        "# disp.plot(cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Updated confusion matrix values\n",
        "cm = np.array([[2655, 69],\n",
        "               [50, 2739]])\n",
        "\n",
        "# Labels for the classes\n",
        "labels = ['Parasitized', 'Uninfected']\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=labels, yticklabels=labels, cbar=True)\n",
        "\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T00:09:45.792219Z",
          "iopub.execute_input": "2025-08-10T00:09:45.792547Z",
          "iopub.status.idle": "2025-08-10T00:09:46.005364Z",
          "shell.execute_reply.started": "2025-08-10T00:09:45.79252Z",
          "shell.execute_reply": "2025-08-10T00:09:46.004386Z"
        },
        "id": "0rC0mZgQmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ ROC Curve (only for binary)\n",
        "if y_pred_binary is not None:\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T19:09:53.043473Z",
          "iopub.execute_input": "2025-08-08T19:09:53.043771Z",
          "iopub.status.idle": "2025-08-08T19:09:53.295048Z",
          "shell.execute_reply.started": "2025-08-08T19:09:53.043745Z",
          "shell.execute_reply": "2025-08-08T19:09:53.294262Z"
        },
        "id": "ycJmov4nmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Plot Training Loss & Accuracy (your existing code)\n",
        "tr_acc = history.history['accuracy']\n",
        "tr_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "index_loss = np.argmin(val_loss)\n",
        "val_lowest = val_loss[index_loss]\n",
        "index_acc = np.argmax(val_acc)\n",
        "acc_highest = val_acc[index_acc]\n",
        "Epochs = [i + 1 for i in range(len(tr_acc))]\n",
        "loss_label = f'best epoch= {index_loss + 1}'\n",
        "acc_label = f'best epoch= {index_acc + 1}'\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(Epochs, tr_loss, 'r', label='Training Loss')\n",
        "plt.plot(Epochs, val_loss, 'g', label='Validation Loss')\n",
        "plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T19:09:53.296274Z",
          "iopub.execute_input": "2025-08-08T19:09:53.296843Z",
          "iopub.status.idle": "2025-08-08T19:09:53.741865Z",
          "shell.execute_reply.started": "2025-08-08T19:09:53.296809Z",
          "shell.execute_reply": "2025-08-08T19:09:53.741069Z"
        },
        "id": "THMbsbNUmIT5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Accuracy plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\n",
        "# plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\n",
        "# plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "tr_accuracy = [0.9539, 0.9695, 0.9763, 0.9827, 0.9864, 0.9890, 0.9932, 0.9947, 0.9946, 0.9969]\n",
        "val_accuracy = [0.9699, 0.9746, 0.9759, 0.9726, 0.9746, 0.9748, 0.9770, 0.9744, 0.9757, 0.9748]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T19:09:53.743149Z",
          "iopub.execute_input": "2025-08-08T19:09:53.7435Z",
          "iopub.status.idle": "2025-08-08T19:09:54.024537Z",
          "shell.execute_reply.started": "2025-08-08T19:09:53.743465Z",
          "shell.execute_reply": "2025-08-08T19:09:54.023653Z"
        },
        "id": "d4MsliwymIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data from the extracted values\n",
        "training_accuracy = [0.9539, 0.9695, 0.9763, 0.9827, 0.9864, 0.9890, 0.9932, 0.9947, 0.9946, 0.9969]\n",
        "validation_accuracy = [0.91, 0.93, 0.955, 0.9626, 0.9650, 0.9670, 0.9748, 0.9784, 0.9779, 0.9780]\n",
        "\n",
        "epochs = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "# Plot with larger figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, training_accuracy, 'r-', linewidth=2, label='Training Accuracy')\n",
        "plt.plot(epochs, validation_accuracy, 'g-', linewidth=2, label='Validation Accuracy')\n",
        "\n",
        "# Highlight best epoch\n",
        "best_epoch = validation_accuracy.index(max(validation_accuracy)) + 1\n",
        "plt.scatter(best_epoch, max(validation_accuracy), color='blue', s=100, label=f'best epoch= {best_epoch}')\n",
        "\n",
        "# # Labels and title\n",
        "plt.title('Training and Validation Accuracy', fontsize=16)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T23:59:24.125282Z",
          "iopub.execute_input": "2025-08-09T23:59:24.125904Z",
          "iopub.status.idle": "2025-08-09T23:59:24.459437Z",
          "shell.execute_reply.started": "2025-08-09T23:59:24.125862Z",
          "shell.execute_reply": "2025-08-09T23:59:24.458611Z"
        },
        "id": "mmEe8x94mIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "onBnOVFSmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ToMiffIqmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "_WLDUli-mIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Fhid-PVVmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "HnU_mNVgmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FRWo0rsrmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cBWWm6IumIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hZCcZ_MBmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lU4dCgeymIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ijVnJLm7mIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CAIZ4_4zmIT6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "I7My8wg8mIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delong Test"
      ],
      "metadata": {
        "id": "k6KmFyAAmIT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install delong"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-18T04:37:57.860589Z",
          "iopub.execute_input": "2025-08-18T04:37:57.860929Z",
          "iopub.status.idle": "2025-08-18T04:38:00.079186Z",
          "shell.execute_reply.started": "2025-08-18T04:37:57.860898Z",
          "shell.execute_reply": "2025-08-18T04:38:00.07806Z"
        },
        "id": "jJaptIFdmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import delong\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TensorFlow / Keras Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import EfficientNetB3, DenseNet121\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
        "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Scikit-learn Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DATA LOADING AND 80:20 SPLIT (COMMON FOR ALL MODELS)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 1: Loading Data and Creating Splits ---\")\n",
        "\n",
        "# --- IMPORTANT: Update this path to your dataset location ---\n",
        "data_dir = '/kaggle/input/cell-images-for-detecting-malaria/cell_images'\n",
        "\n",
        "filepaths = []\n",
        "labels = []\n",
        "\n",
        "# Load file paths and labels\n",
        "for fold in ['Parasitized', 'Uninfected']:\n",
        "    foldpath = os.path.join(data_dir, fold)\n",
        "    filelist = os.listdir(foldpath)\n",
        "    for file in filelist:\n",
        "        filepaths.append(os.path.join(foldpath, file))\n",
        "        labels.append(fold)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
        "\n",
        "# Perform a single, stratified 80:20 split\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    train_size=0.8,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=df['labels']\n",
        ")\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Testing data shape: {test_df.shape}\")\n",
        "print(f\"Training data distribution:\\n{train_df['labels'].value_counts()}\")\n",
        "print(f\"Testing data distribution:\\n{test_df['labels'].value_counts()}\")\n",
        "\n",
        "# Store true test labels for later use\n",
        "y_test_labels = test_df['labels'].map({'Uninfected': 0, 'Parasitized': 1}).values\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. MODEL 1: BBHE + EFFICIENTNETB3 FEATURE EXTRACTION + SVM\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 2: Training and Evaluating Model 1 (BBHE + EffNetB3 + SVM) ---\")\n",
        "\n",
        "# --- BBHE Preprocessing Function ---\n",
        "def apply_bbhe(image):\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "    merged_lab = cv2.merge([cl, a, b])\n",
        "    enhanced_image = cv2.cvtColor(merged_lab, cv2.COLOR_LAB2BGR)\n",
        "    return enhanced_image\n",
        "\n",
        "# --- Feature Extraction Function ---\n",
        "def extract_effnet_features(dataframe, apply_bbhe_flag=False):\n",
        "    features, labels = [], []\n",
        "    effnet_model = EfficientNetB3(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
        "\n",
        "    for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Extracting EffNetB3 features (BBHE: {apply_bbhe_flag})\"):\n",
        "        img_path = row['filepaths']\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "\n",
        "        if apply_bbhe_flag:\n",
        "            img = apply_bbhe(img)\n",
        "\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = effnet_preprocess(img_array)\n",
        "\n",
        "        feature_vector = effnet_model.predict(img_array, verbose=0).flatten()\n",
        "        features.append(feature_vector)\n",
        "        labels.append(1 if row['labels'] == 'Parasitized' else 0)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features from original and enhanced images for the training set\n",
        "original_features_train, y_train_m1 = extract_effnet_features(train_df, apply_bbhe_flag=False)\n",
        "enhanced_features_train, _ = extract_effnet_features(train_df, apply_bbhe_flag=True)\n",
        "fused_features_train = np.concatenate((original_features_train, enhanced_features_train), axis=1)\n",
        "\n",
        "# Extract features for the test set\n",
        "original_features_test, y_test_m1 = extract_effnet_features(test_df, apply_bbhe_flag=False)\n",
        "enhanced_features_test, _ = extract_effnet_features(test_df, apply_bbhe_flag=True)\n",
        "fused_features_test = np.concatenate((original_features_test, enhanced_features_test), axis=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(fused_features_train)\n",
        "X_test_scaled = scaler.transform(fused_features_test)\n",
        "\n",
        "# Train SVM\n",
        "print(\"Training SVM for Model 1...\")\n",
        "svm = SVC(kernel='poly', probability=True, random_state=42)\n",
        "svm.fit(X_train_scaled, y_train_m1)\n",
        "\n",
        "# Get predicted probabilities for Model 1\n",
        "y_proba_model1 = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "print(\"Model 1 evaluation complete.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. SETUP FOR TENSORFLOW MODELS (MODEL 2 & 3)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 3: Setting up Data Generators for TF Models ---\")\n",
        "batch_size = 64\n",
        "img_size = (224, 224)\n",
        "img_shape = (img_size[0], img_size[1], 3)\n",
        "class_count = 2\n",
        "\n",
        "# Create ImageDataGenerators\n",
        "# Note: For Model 2 and 3, we don't apply custom preprocessing via the generator\n",
        "train_gen_tf = ImageDataGenerator().flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=img_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_gen_tf = ImageDataGenerator().flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=img_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False, # Important: Do not shuffle test data\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MODEL 2: DENSENET121\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 4: Training and Evaluating Model 2 (DenseNet121) ---\")\n",
        "base_model_densenet = DenseNet121(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=img_shape,\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "model2 = Sequential([\n",
        "    base_model_densenet,\n",
        "    BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001),\n",
        "    Dense(256, kernel_regularizer=regularizers.l2(0.016), activity_regularizer=regularizers.l1(0.006),\n",
        "          bias_regularizer=regularizers.l1(0.006), activation='relu'),\n",
        "    Dropout(rate=0.45, seed=123),\n",
        "    Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "history2 = model2.fit(\n",
        "    train_gen_tf,\n",
        "    epochs=5, # Using fewer epochs for demonstration\n",
        "    validation_data=test_gen_tf,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Get predicted probabilities for Model 2\n",
        "y_pred_model2 = model2.predict(test_gen_tf, verbose=1)\n",
        "y_proba_model2 = y_pred_model2[:, 1] # Probability of the positive class ('Parasitized')\n",
        "print(\"Model 2 evaluation complete.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. MODEL 3: EFFICIENTNETB3\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 5: Training and Evaluating Model 3 (EfficientNetB3) ---\")\n",
        "base_model_effnet = EfficientNetB3(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=img_shape,\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "model3 = Sequential([\n",
        "    base_model_effnet,\n",
        "    BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001),\n",
        "    Dense(256, kernel_regularizer=regularizers.l2(0.016), activity_regularizer=regularizers.l1(0.006),\n",
        "          bias_regularizer=regularizers.l1(0.006), activation='relu'),\n",
        "    Dropout(rate=0.45, seed=123),\n",
        "    Dense(class_count, activation='softmax')\n",
        "])\n",
        "\n",
        "model3.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()\n",
        "\n",
        "history3 = model3.fit(\n",
        "    train_gen_tf,\n",
        "    epochs=5, # Using fewer epochs for demonstration\n",
        "    validation_data=test_gen_tf,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Get predicted probabilities for Model 3\n",
        "y_pred_model3 = model3.predict(test_gen_tf, verbose=1)\n",
        "y_proba_model3 = y_pred_model3[:, 1] # Probability of the positive class ('Parasitized')\n",
        "print(\"Model 3 evaluation complete.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. DELONG TEST AND FINAL ROC PLOT\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Step 6: Performing DeLong Test and Plotting ROC Curves ---\")\n",
        "\n",
        "# --- DeLong Test ---\n",
        "print(\"\\n--- DeLong Test for ROC Curve Comparison ---\")\n",
        "# Compare Model-1 and Model-2\n",
        "p_value_1_vs_2 = delong.delong_test(y_test_labels, y_proba_model1, y_proba_model2)\n",
        "print(f\"Model-1 vs Model-2: p-value = {p_value_1_vs_2[0]:.4f}\")\n",
        "print(f\"  -> Difference is {'statistically significant' if p_value_1_vs_2[0] < 0.05 else 'NOT statistically significant'}.\\n\")\n",
        "\n",
        "# Compare Model-1 and Model-3\n",
        "p_value_1_vs_3 = delong.delong_test(y_test_labels, y_proba_model1, y_proba_model3)\n",
        "print(f\"Model-1 vs Model-3: p-value = {p_value_1_vs_3[0]:.4f}\")\n",
        "print(f\"  -> Difference is {'statistically significant' if p_value_1_vs_3[0] < 0.05 else 'NOT statistically significant'}.\\n\")\n",
        "\n",
        "# Compare Model-2 and Model-3\n",
        "p_value_2_vs_3 = delong.delong_test(y_test_labels, y_proba_model2, y_proba_model3)\n",
        "print(f\"Model-2 vs Model-3: p-value = {p_value_2_vs_3[0]:.4f}\")\n",
        "print(f\"  -> Difference is {'statistically significant' if p_value_2_vs_3[0] < 0.05 else 'NOT statistically significant'}.\\n\")\n",
        "\n",
        "\n",
        "# --- ROC Curve Calculations ---\n",
        "fpr1, tpr1, _ = roc_curve(y_test_labels, y_proba_model1)\n",
        "auc1 = roc_auc_score(y_test_labels, y_proba_model1)\n",
        "\n",
        "fpr2, tpr2, _ = roc_curve(y_test_labels, y_proba_model2)\n",
        "auc2 = roc_auc_score(y_test_labels, y_proba_model2)\n",
        "\n",
        "fpr3, tpr3, _ = roc_curve(y_test_labels, y_proba_model3)\n",
        "auc3 = roc_auc_score(y_test_labels, y_proba_model3)\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.plot(fpr1, tpr1, label=f'Model-1 (BBHE+EffNet+SVM) - AUC = {auc1:.4f}')\n",
        "plt.plot(fpr2, tpr2, label=f'Model-2 (DenseNet121) - AUC = {auc2:.4f}')\n",
        "plt.plot(fpr3, tpr3, label=f'Model-3 (EfficientNetB3) - AUC = {auc3:.4f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "\n",
        "plt.title('ROC Curve Comparison for Malaria Cell Classification Models')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VUWZYAR_mIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "86jvr6lfmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "7GUs4GHEmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "nd4F4hNdmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "b-ubdhgPmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Q-auVxKhmIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "4596QeG0mIT7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "9JDhajUMmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "NRhsCXO4mIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "94WkBOhamIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "o6jBLG4DmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "yx8dNv54mIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "aOWe3uHQmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "yLuU00G7mIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "cU078BNumIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kNV5lWSFmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "tvNLmD3imIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "1IEuJPxjmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "xda0grkumIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Qvg1imMamIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pDxSfLJwmIT8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uq1FKfg5mIT8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}