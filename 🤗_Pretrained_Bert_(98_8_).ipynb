{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ðŸ¤— Pretrained-Bert (98.8%)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesis17/Afaan-Oromoo-chatGPT/blob/main/%F0%9F%A4%97_Pretrained_Bert_(98_8_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "niyarrbarman_symptom2disease_path = kagglehub.dataset_download('niyarrbarman/symptom2disease')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CtW0yAhRZCv3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "\n",
        "The task is to create a language model that can effectively classify diseases based on a short description of its symptoms.\n",
        "\n",
        "\n",
        "The dataset contains 50 descriptions for each disease, which turns out too less to train a model from scratch. Hence, I tried using a pretrained\n",
        "transformer model for this. Thanks to ðŸ¤—.\n"
      ],
      "metadata": {
        "id": "tyWRDpG5ZCv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "i4YY5xezZCv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sea\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "np.__version__"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-04-06T03:59:13.655065Z",
          "iopub.execute_input": "2023-04-06T03:59:13.655876Z",
          "iopub.status.idle": "2023-04-06T03:59:21.494627Z",
          "shell.execute_reply.started": "2023-04-06T03:59:13.655836Z",
          "shell.execute_reply": "2023-04-06T03:59:21.493464Z"
        },
        "trusted": true,
        "id": "yW5xCQrjZCv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T03:59:21.496735Z",
          "iopub.execute_input": "2023-04-06T03:59:21.497537Z",
          "iopub.status.idle": "2023-04-06T03:59:32.103215Z",
          "shell.execute_reply.started": "2023-04-06T03:59:21.497495Z",
          "shell.execute_reply": "2023-04-06T03:59:32.101959Z"
        },
        "trusted": true,
        "id": "JTpcVzJzZCv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "-jkyBplzZCv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/symptom2disease/Symptom2Disease.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T03:59:32.106128Z",
          "iopub.execute_input": "2023-04-06T03:59:32.106453Z",
          "iopub.status.idle": "2023-04-06T03:59:32.150097Z",
          "shell.execute_reply.started": "2023-04-06T03:59:32.106422Z",
          "shell.execute_reply": "2023-04-06T03:59:32.148543Z"
        },
        "trusted": true,
        "id": "a-YxUQg5ZCv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:00:25.115277Z",
          "iopub.execute_input": "2023-04-06T04:00:25.11579Z",
          "iopub.status.idle": "2023-04-06T04:00:25.13152Z",
          "shell.execute_reply.started": "2023-04-06T04:00:25.115748Z",
          "shell.execute_reply": "2023-04-06T04:00:25.130436Z"
        },
        "trusted": true,
        "id": "nI6rjzB1ZCv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating encodings for disease."
      ],
      "metadata": {
        "id": "w_ElydyeZCv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int2label = {}\n",
        "\n",
        "for i, disease in enumerate(df['label'].unique()):\n",
        "    int2label[i] = disease\n",
        "\n",
        "label2int = {v : k for k, v in int2label.items()}\n",
        "num_classes = len(int2label)\n",
        "\n",
        "int2label, label2int"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:00:31.856493Z",
          "iopub.execute_input": "2023-04-06T04:00:31.856877Z",
          "iopub.status.idle": "2023-04-06T04:00:31.867439Z",
          "shell.execute_reply.started": "2023-04-06T04:00:31.856842Z",
          "shell.execute_reply": "2023-04-06T04:00:31.866304Z"
        },
        "trusted": true,
        "id": "7ptqT78EZCv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Distribution\n",
        "\n",
        "The dataset contains 24 diseases and each disease has 50 descriptions of the respective symptoms."
      ],
      "metadata": {
        "id": "1w3QUCVNZCv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "\n",
        "sea.countplot(y = df['label'], palette = 'Blues_d')\n",
        "plt.title('Count of each disease')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:03:37.613638Z",
          "iopub.execute_input": "2023-04-06T04:03:37.614019Z",
          "iopub.status.idle": "2023-04-06T04:03:38.020699Z",
          "shell.execute_reply.started": "2023-04-06T04:03:37.613988Z",
          "shell.execute_reply": "2023-04-06T04:03:38.01971Z"
        },
        "trusted": true,
        "id": "PDuYUC0xZCv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution of the lenght of description"
      ],
      "metadata": {
        "id": "9-Vg4KpeZCv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = []\n",
        "for txt in tqdm(df['text'].values):\n",
        "    length.append(len(txt.split()))\n",
        "\n",
        "sea.histplot(length, kde = True, bins = 20)\n",
        "plt.title('Distribution of description length')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:03:21.574432Z",
          "iopub.execute_input": "2023-04-06T04:03:21.575107Z",
          "iopub.status.idle": "2023-04-06T04:03:21.882331Z",
          "shell.execute_reply.started": "2023-04-06T04:03:21.575072Z",
          "shell.execute_reply": "2023-04-06T04:03:21.88137Z"
        },
        "trusted": true,
        "id": "Fo529yPtZCv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['label'].map(lambda x : label2int[x])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:03:43.674217Z",
          "iopub.execute_input": "2023-04-06T04:03:43.675236Z",
          "iopub.status.idle": "2023-04-06T04:03:43.682144Z",
          "shell.execute_reply.started": "2023-04-06T04:03:43.675198Z",
          "shell.execute_reply": "2023-04-06T04:03:43.680998Z"
        },
        "trusted": true,
        "id": "JSxQ1KwDZCv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df['text'].values, df['label'].values\n",
        "\n",
        "x_tokenizer = Tokenizer(filters = '')\n",
        "x_tokenizer.fit_on_texts(X)\n",
        "x_vocab = len(x_tokenizer.word_index) + 1\n",
        "print(\"X vocab:\", x_vocab)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:04:28.014929Z",
          "iopub.execute_input": "2023-04-06T04:04:28.015912Z",
          "iopub.status.idle": "2023-04-06T04:04:28.054124Z",
          "shell.execute_reply.started": "2023-04-06T04:04:28.015859Z",
          "shell.execute_reply": "2023-04-06T04:04:28.053048Z"
        },
        "trusted": true,
        "id": "jrNfJbwDZCv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the train and testing data\n",
        "\n",
        "90% of the dataset is used for training and remaining 10% for validation and testing."
      ],
      "metadata": {
        "id": "obE6bfKDZCv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size = 0.1, stratify = y)\n",
        "train_x.shape, val_x.shape, train_y.shape, val_y.shape\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:07:25.190099Z",
          "iopub.execute_input": "2023-04-06T04:07:25.19058Z",
          "iopub.status.idle": "2023-04-06T04:07:25.200693Z",
          "shell.execute_reply.started": "2023-04-06T04:07:25.190535Z",
          "shell.execute_reply": "2023-04-06T04:07:25.19964Z"
        },
        "trusted": true,
        "id": "y7ihy_zbZCv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the data into `tf.data.Dataset`\n",
        "\n",
        "1. The tokenizer of pretrained bert is downloaded and used.\n",
        "2. `train_dataset` and `val_dataset` is created using `tf.data.Dataset` with batch size 8."
      ],
      "metadata": {
        "id": "5U5yOZUjZCv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "train_encodings = tokenizer(list(train_x), padding=\"max_length\", truncation=True)\n",
        "val_encodings = tokenizer(list(val_x), padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_y\n",
        ")).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_y\n",
        ")).batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:07:26.753828Z",
          "iopub.execute_input": "2023-04-06T04:07:26.754701Z",
          "iopub.status.idle": "2023-04-06T04:07:34.552516Z",
          "shell.execute_reply.started": "2023-04-06T04:07:26.75466Z",
          "shell.execute_reply": "2023-04-06T04:07:34.551429Z"
        },
        "trusted": true,
        "id": "2fzCOG8PZCv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras --upgrade"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:07:34.554415Z",
          "iopub.execute_input": "2023-04-06T04:07:34.554801Z",
          "iopub.status.idle": "2023-04-06T04:07:44.193894Z",
          "shell.execute_reply.started": "2023-04-06T04:07:34.554763Z",
          "shell.execute_reply": "2023-04-06T04:07:44.192491Z"
        },
        "trusted": true,
        "id": "V9f2hklyZCwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Pretrained BERT Model\n",
        "\n",
        "1. We pass the label encodings to the model.\n",
        "2. We use SparseCategoricalCrossentropy for the loss function.\n",
        "3. Adam for the optimizer with a small learning rate of 0.00003\n"
      ],
      "metadata": {
        "id": "meYYFeGEZCwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 24\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels = num_classes,\n",
        "    id2label = int2label,\n",
        "    label2id = label2int,\n",
        "    output_attentions = True)\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 3e-5),\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:09:24.274913Z",
          "iopub.execute_input": "2023-04-06T04:09:24.275911Z",
          "iopub.status.idle": "2023-04-06T04:09:34.787281Z",
          "shell.execute_reply.started": "2023-04-06T04:09:24.275865Z",
          "shell.execute_reply": "2023-04-06T04:09:34.785726Z"
        },
        "trusted": true,
        "id": "E0UztEOcZCwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:09:34.789771Z",
          "iopub.execute_input": "2023-04-06T04:09:34.790166Z",
          "iopub.status.idle": "2023-04-06T04:09:34.804867Z",
          "shell.execute_reply.started": "2023-04-06T04:09:34.790124Z",
          "shell.execute_reply": "2023-04-06T04:09:34.802726Z"
        },
        "trusted": true,
        "id": "sjn2xJoNZCwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning the model\n",
        "\n",
        "1. The model is fined tuned for 3 epochs."
      ],
      "metadata": {
        "id": "9MU5eMr_ZCwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "          epochs = EPOCHS,\n",
        "          validation_data = val_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-06T04:10:56.244568Z",
          "iopub.execute_input": "2023-04-06T04:10:56.244959Z"
        },
        "trusted": true,
        "id": "1T2A16nRZCwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the loss and accuracy plot."
      ],
      "metadata": {
        "id": "WszGwQEvZCwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (14, 5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], marker = 'o', label = 'Training')\n",
        "plt.plot(history.history['val_loss'], marker='o', label = 'Validation')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], marker = 'o', label = 'Training')\n",
        "plt.plot(history.history['val_accuracy'], marker='o', label = 'Validation')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "MAhubhLHZCwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on custom input."
      ],
      "metadata": {
        "id": "Bg0O_wTbZCwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k = num_classes)\n",
        "pred1 = pipe(\"I am experiencing rashes on my skin. It is itchy and is now swelling. Even my sking is starting to peel. \")\n",
        "pred2 = pipe(\"I have constipation and belly pain, and it's been really uncomfortable. The belly pain has been getting worse and is starting to affect my daily life. Moreover, I get chills every night, followed by a mild fever.\")\n",
        "\n",
        "\n",
        "print(pred1[0][:2])\n",
        "print(pred2[0][:2])"
      ],
      "metadata": {
        "trusted": true,
        "id": "rTrOe5UfZCwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AIK1zZEZCwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}