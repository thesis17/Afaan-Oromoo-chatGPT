{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Medical Symptoms Text Classification + Web App",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesis17/Afaan-Oromoo-chatGPT/blob/main/Medical_Symptoms_Text_Classification_%2B_Web_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "paultimothymooney_medical_speech_transcription_and_intent_path = kagglehub.dataset_download('paultimothymooney/medical-speech-transcription-and-intent')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZUumewXx7oh5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Symptoms Text Classification\n",
        "\n",
        "> The adoption of natural language processing in healthcare is rising because of its recognized potential to search, analyze and interpret mammoth amounts of patient datasets. Using advanced medical algorithms, machine learning in healthcare and NLP technology services have the potential to harness relevant insights and concepts from data that was previously considered buried in text form. NLP in healthcare media can accurately give voice to the unstructured data of the healthcare universe, giving incredible insight into understanding quality, improving methods, and better results for patients.\n",
        "\n",
        "![](https://i.imgur.com/SJPzebD.png)\n",
        "\n",
        "The web app is available at: https://medical-symptoms-classifier.herokuapp.com/\n",
        "\n",
        "Source Code: https://github.com/gabbygab1233/Medical-Symptoms-Classifier"
      ],
      "metadata": {
        "id": "fRnzItBX7oh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspellchecker\n",
        "#!pip install neattext"
      ],
      "metadata": {
        "trusted": true,
        "id": "S7t-zafL7oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import altair as alt\n",
        "import pickle\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from sklearn.naive_bayes import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.neighbors import *\n",
        "from sklearn.tree import *\n",
        "from sklearn.calibration import *\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.multiclass import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "sns.set(style='whitegrid')\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('../input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ttk03c8j7oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Exploratory Data Analysis**](http://)"
      ],
      "metadata": {
        "id": "xC6BRW5x7oh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyze Data\n",
        "def explore_data(df):\n",
        "    print(f\"The data contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "    print('\\n')\n",
        "    print('Dataset columns:',df.columns)\n",
        "    print('\\n')\n",
        "    print(df.info())\n",
        "\n",
        "explore_data(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VjsMJDa-7oh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Checking for Nan Values and duplicates**Â¶](http://)"
      ],
      "metadata": {
        "id": "akr7gfsu7oh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "zHf9avFp7oh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checking_removing_duplicates(df):\n",
        "    count_dups = df.duplicated().sum()\n",
        "    print(\"Number of Duplicates: \", count_dups)\n",
        "    if count_dups >= 1:\n",
        "        df.drop_duplicates(inplace=True)\n",
        "        print('Duplicate values removed!')\n",
        "    else:\n",
        "        print('No Duplicate values')\n",
        "checking_removing_duplicates(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5rXvHIt27oiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Different types of analysis require different data format**](http://)\n",
        "* **Corpus** - a collection of text\n",
        "* **Document-Term Matrix** - word counts in matrix format"
      ],
      "metadata": {
        "id": "DXPZQ5Y37oiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Corpus**"
      ],
      "metadata": {
        "id": "Xtkj57jk7oiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_text = df[['phrase', 'prompt']]\n",
        "df_text"
      ],
      "metadata": {
        "trusted": true,
        "id": "663fiFmD7oiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Document-Term Matrix**"
      ],
      "metadata": {
        "id": "dga_J68D7oiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()\n",
        "df_cv = cv.fit_transform(df_text.phrase)\n",
        "data_dtm = pd.DataFrame(df_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_dtm.index = df_text.index\n",
        "data_dtm\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "p-YTgz2-7oiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add features\n",
        "# Number of characters in the text\n",
        "df_text['phrase_length'] = df_text['phrase'].apply(len)\n",
        "# Number of words in the text\n",
        "df_text['phrase_num_words'] = df_text['phrase'].apply(lambda x: len(x.split()))\n",
        "# Average length of the words in the text\n",
        "df_text[\"mean_word_len\"] = df_text[\"phrase\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "# Number of non-stopwords in the text\n",
        "df_text['phrase_non_stopwords'] = df_text['phrase'].apply(lambda x: len([t for t in x.split() if t not in STOP_WORDS]))\n",
        "df_text.describe().T"
      ],
      "metadata": {
        "trusted": true,
        "id": "zOHBBNQK7oiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_dist = df_text['prompt'].value_counts().reset_index().rename(columns={'index':'Label', 'prompt':'Count'})\n",
        "cat_dist.drop(0, axis=0, inplace=True)\n",
        "alt.Chart(cat_dist).mark_bar(opacity=0.7).encode(\n",
        "    x=alt.X('Count', title='Count'),\n",
        "    y=alt.Y('Label', sort='-x',title='Category'),\n",
        "    tooltip=['Label','Count']\n",
        ").properties(height=800,width=700,title=\"Class Distribution\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "ZzrlHXBT7oiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = df_text['prompt'].values\n",
        "counter = Counter(target)\n",
        "for k,v in counter.items():\n",
        "    per = v / len(target) * 100\n",
        "    print('Class=%s, Count=%d, Percentage=%.3f%%' % (k, v, per))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "hcshZ__a7oiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.data_transformers.disable_max_rows()\n",
        "alt.Chart(df_text).mark_bar(color=\"violet\",opacity=0.7,\n",
        "    interpolate='step').encode(\n",
        "    alt.X(\"phrase_length:Q\",  bin=alt.Bin(maxbins=100), title='Phrase Length Class'),\n",
        "    alt.Y('count()', axis=alt.Axis(labels=False), title='Frequency'),\n",
        "    tooltip=['phrase_length']\n",
        ").properties(\n",
        "    height=400,\n",
        "    width=700, title=\"Length Distribution\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "yHHeB0qY7oiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Text Cleaning**](http://)\n",
        "You cannot go straight from raw text to fitting a machine learning or deep learning model. You\n",
        "must clean your text first, which means splitting it into words and handling punctuation and\n",
        "case.\n",
        "* Load the raw text\n",
        "\n",
        "* Split into tokens\n",
        "\n",
        "* Convert to lowercase\n",
        "\n",
        "* Remove punctuation from each token.\n",
        "\n",
        "* Filter out remaining tokens that are not alphabetic.\n",
        "\n",
        "* Filter out tokens that are stop words\n",
        "\n",
        "* Filter out short tokens by checking their length\n",
        "\n",
        "## **Additional Text Cleaning Considerations**\n",
        "* Extracting text from markup like HTML, PDF, or other structured document formats.\n",
        "\n",
        "* Transliteration of characters from other languages into English.\n",
        "\n",
        "* Decoding Unicode characters into a normalized form, such as UTF8.\n",
        "\n",
        "* Handling of domain specific words, phrases, and acronyms.\n",
        "\n",
        "* Handling or removing numbers, such as dates and amounts.\n",
        "\n",
        "* Locating and correcting common typos and misspellings."
      ],
      "metadata": {
        "id": "7xmf1k4s7oiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_txt(docs):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # split into words\n",
        "    speech_words = nltk.word_tokenize(docs)\n",
        "    # convert to lower case\n",
        "    lower_text = [w.lower() for w in speech_words]\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    stripped = [re_punc.sub('', w) for w in lower_text]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    words = [w for w in words if not w in  list(STOP_WORDS)]\n",
        "    # filter out short tokens\n",
        "    words = [word for word in words if len(word) > 2]\n",
        "    #Stemm all the words in the sentence\n",
        "    lem_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    combined_text = ' '.join(lem_words)\n",
        "    return combined_text\n",
        "\n",
        "# Cleaning the text data\n",
        "df_text['cleaned_phrase'] = df_text['phrase'].apply(clean_txt)\n",
        "df_text"
      ],
      "metadata": {
        "trusted": true,
        "id": "TTGdlPFB7oiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_splits = FreqDist(df_text['phrase'])\n",
        "print(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")"
      ],
      "metadata": {
        "id": "V-dJeetb7oiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Wordcloud for each class**](http://)"
      ],
      "metadata": {
        "id": "sG5nbY-u7oiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "# generate word cloud and show it\n",
        "\n",
        "for x in df_text.prompt.unique():\n",
        "    wc = WordCloud(background_color=\"black\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)\n",
        "    wc.generate(df_text.cleaned_phrase[(df_text.cleaned_phrase.notnull()) & (df_text.prompt == x)].to_string())\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.title(x, fontdict={'size':16,'weight':'bold'})\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "7V9RnO0B7oiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Text Data Preparation and Model Training](http://)\n",
        "Text data requires special preparation before you can start using it for predictive modeling. The\n",
        "text must be parsed to remove words, called tokenization. Then the words need to be encoded\n",
        "as integers or floating point values for use as input to a machine learning algorithm, called\n",
        "feature extraction (or vectorization).\n"
      ],
      "metadata": {
        "id": "w4up8m4i7oiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spot-Check Normalized Text Models\n",
        "def NormalizedTextModel(nameOfvect):\n",
        "    if nameOfvect == 'countvect':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif nameOfvect =='tfvect':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif nameOfvect == 'hashvect':\n",
        "        vectorizer = HashingVectorizer()\n",
        "\n",
        "    pipelines = []\n",
        "    pipelines.append((nameOfvect+'MultinomialNB'  , Pipeline([('Vectorizer', vectorizer),('NB'  , MultinomialNB())])))\n",
        "    pipelines.append((nameOfvect+'CCCV' , Pipeline([('Vectorizer', vectorizer),('CCCV' , CalibratedClassifierCV())])))\n",
        "    pipelines.append((nameOfvect+'KNN' , Pipeline([('Vectorizer', vectorizer),('KNN' , KNeighborsClassifier())])))\n",
        "    pipelines.append((nameOfvect+'CART', Pipeline([('Vectorizer', vectorizer),('CART', DecisionTreeClassifier())])))\n",
        "    pipelines.append((nameOfvect+'PAC'  , Pipeline([('Vectorizer', vectorizer),('PAC'  , PassiveAggressiveClassifier())])))\n",
        "    pipelines.append((nameOfvect+'SVM' , Pipeline([('Vectorizer', vectorizer),('RC' , RidgeClassifier())])))\n",
        "    pipelines.append((nameOfvect+'AB'  , Pipeline([('Vectorizer', vectorizer),('AB'  , AdaBoostClassifier())])  ))\n",
        "    pipelines.append((nameOfvect+'GBM' , Pipeline([('Vectorizer', vectorizer),('GMB' , GradientBoostingClassifier())])))\n",
        "    pipelines.append((nameOfvect+'RF'  , Pipeline([('Vectorizer', vectorizer),('RF'  , RandomForestClassifier())])))\n",
        "    pipelines.append((nameOfvect+'ET'  , Pipeline([('Vectorizer', vectorizer),('ET'  , ExtraTreesClassifier())])))\n",
        "    pipelines.append((nameOfvect+'SGD'  , Pipeline([('Vectorizer', vectorizer),('SGD'  , SGDClassifier())])))\n",
        "    pipelines.append((nameOfvect+'OVRC'  , Pipeline([('Vectorizer', vectorizer),('OVRC'  , OneVsRestClassifier(LogisticRegression()))])))\n",
        "    pipelines.append((nameOfvect+'Bagging'  , Pipeline([('Vectorizer', vectorizer),('Bagging'  , BaggingClassifier())])))\n",
        "    pipelines.append((nameOfvect+'NN'  , Pipeline([('Vectorizer', vectorizer),('NN'  , MLPClassifier())])))\n",
        "    #pipelines.append((nameOfvect+'xgboost', Pipeline([('Vectorizer', vectorizer), ('xgboost', XGBClassifier())])))\n",
        "    return pipelines\n",
        "\n",
        "# Traing model\n",
        "def fit_model(X_train, y_train,models):\n",
        "    # Test options and evaluation metric\n",
        "    num_folds = 10\n",
        "    scoring = 'accuracy'\n",
        "\n",
        "    results = []\n",
        "    names = []\n",
        "    for name, model in models:\n",
        "        kfold = KFold(n_splits=num_folds)\n",
        "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "        results.append(cv_results)\n",
        "        names.append(name)\n",
        "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "        print(msg)\n",
        "\n",
        "# Split data to training and validation set\n",
        "def read_in_and_split_data(data, features,target):\n",
        "    X = data[features]\n",
        "    y = data[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X = 'cleaned_phrase'\n",
        "target_class = 'prompt'\n",
        "X_train, X_test, y_train, y_test = read_in_and_split_data(df_text, X, target_class)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vyiPNeYi7oiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Bag of Words Model**](http://)\n",
        "We cannot work with text directly when using machine learning algorithms. Instead, we need\n",
        "to convert the text to numbers. We may want to perform classification of documents, so each\n",
        "document is an input and a class label is the output for our predictive algorithm. Algorithms\n",
        "take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors\n",
        "of numbers."
      ],
      "metadata": {
        "id": "WTdA_hsM7oiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Word Counts with countvectorizer** ](http://)\n",
        "- The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary."
      ],
      "metadata": {
        "id": "o2uhSHS47oiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "sample_text_count = X_train[:10]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(sample_text_count)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(sample_text_count)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dsa0FhtB7oiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Spot-Check Algorithms with Countvectorizer**](http://)"
      ],
      "metadata": {
        "id": "aON3qNTt7oiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contvectorizer\n",
        "models = NormalizedTextModel('countvect')\n",
        "fit_model(X_train, y_train, models)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wU84x4AV7oiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Word Frequencies with TfidfVectorizer** ](http://)\n",
        "- Word counts are a good starting point, but are very basic. One issue with simple counts is that some words like the will appear many times and their large counts will not be very meaningful in the encoded vectors. An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF.\n",
        "\t* **Term Frequency**: This summarizes how often a given word appears within a document.\n",
        "\t* **Inverse Document Frequency**: This downscales words that appear a lot across documents."
      ],
      "metadata": {
        "id": "DLmYZ1sT7oiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "sample_text_Tfid = X_train[:10]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(sample_text_Tfid)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(sample_text_Tfid)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "trusted": true,
        "id": "EjodWkV-7oiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Spot-Check Algorithms with TfidfVectorizer**](http://)"
      ],
      "metadata": {
        "id": "xvKSaRD67oiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer\n",
        "models = NormalizedTextModel('tfvect')\n",
        "fit_model(X_train, y_train, models)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tcX6eYwZ7oiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Hashing with HashingVectorizer**](http://)\n",
        "- Counts and frequencies can be very useful, but one limitation of these methods is that the\n",
        "vocabulary can become very large. This, in turn, will require large vectors for encoding\n",
        "documents and impose large requirements on memory and slow down algorithms. A clever work\n",
        "around is to use a one way hash of words to convert them to integers. The clever part is that\n",
        "no vocabulary is required and you can choose an arbitrary-long fixed length vector."
      ],
      "metadata": {
        "id": "BtBeWuNo7oiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text\n",
        "sample_text_hash = X_train[:10]\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "# encode document\n",
        "vector = vectorizer.transform(sample_text_hash)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "trusted": true,
        "id": "UNy1vyOM7oiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Spot-Check Algorithms  with HashingVectorizer**](http://)"
      ],
      "metadata": {
        "id": "qQ8Ky_5d7oiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer\n",
        "#models = NormalizedTextModel('hashvect')\n",
        "#fit_model(X_train, y_train, models)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8HCCrEdh7oiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Fine tuning**](http://)"
      ],
      "metadata": {
        "id": "TjXuiaEZ7oiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train_1 = vectorizer.fit_transform(X_train)\n",
        "model = BaggingClassifier()\n",
        "n_estimators = [10, 100, 1000]\n",
        "#learning_rate= [0.1, 0.001, 0.0001]\n",
        "#max_depth = [4,5,6]\n",
        "#min_child_weight=[4,5,6]\n",
        "\n",
        "#define grid search\n",
        "grid = dict(n_estimators=n_estimators)\n",
        "cv = KFold(n_splits=10)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
        "grid_result = grid_search.fit(X_train_1, y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "trusted": true,
        "id": "bLabdztF7oiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Predict unseen data**](http://)"
      ],
      "metadata": {
        "id": "m5PpRd1U7oiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_metrics(model, y_test, y_pred):\n",
        "    print(f\"Training Accuracy Score: {model.score(X_train, y_train) * 100:.1f}%\")\n",
        "    print(f\"Validation Accuracy Score: {model.score(X_test, y_test) * 100:.1f}%\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    fig,ax = plt.subplots(figsize=(8,6))\n",
        "    sns.heatmap(pd.DataFrame(conf_matrix), annot = True, cmap = 'YlGnBu',fmt = 'g')\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    plt.tight_layout()\n",
        "    plt.title('Confusion matrix for Logisitic Regression Model', fontsize=20, y=1.1)\n",
        "    plt.ylabel('Actual label', fontsize=15)\n",
        "    plt.xlabel('Predicted label', fontsize=15)\n",
        "    plt.show()\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "text_clf = Pipeline([('vect', TfidfVectorizer()),('bagging', BaggingClassifier(n_estimators=10))])\n",
        "model = text_clf.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "classification_metrics(model,y_test, y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6tWeIJs77oiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "* https://www.foreseemed.com/natural-language-processing-in-healthcare\n",
        "* https://appen.com/datasets/audio-recording-and-transcription-for-medical-scenarios/\n",
        "* https://www.kaggle.com/paultimothymooney/medical-speech-transcription-and-intent"
      ],
      "metadata": {
        "id": "O5qtImKz7oiK"
      }
    }
  ]
}